{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f559b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"cyberthreat-pathfinder-notebook.ipynb\n",
    "\n",
    "Automatically generated by Colab.\n",
    "\n",
    "Original file is located at\n",
    "    https://colab.research.google.com/drive/1r5NiN210Lm73pXlanKmmdw6SY6oNEvNE\n",
    "\n",
    "CyberThreat PathFinder: An Agentic Graph Intelligence System\n",
    "\n",
    "# Setup and Environment\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609ef822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install nx-arangodb via pip\n",
    "!pip install nx-arangodb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3ecc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if you have an NVIDIA GPU\n",
    "# Note: If this returns \"command not found\", then GPU-based algorithms via cuGraph are unavailable\n",
    "!nvidia-smi\n",
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd42408e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install nx-cugraph via pip (if GPU is available)\n",
    "!pip install nx-cugraph-cu12 --extra-index-url=https://pypi.nvidia.com # Requires CUDA-capable GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fd034d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install cuGraph\n",
    "!pip install cugraph-cu12 --extra-index-url=https://pypi.nvidia.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ceb064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary dependencies\n",
    "!pip install langchain langchain-community langgraph ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24340e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Commented out IPython magic to ensure Python compatibility.\n",
    "!pip install colab-xterm\n",
    "%load_ext colabxterm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6dfe549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Commented out IPython magic to ensure Python compatibility.\n",
    "%xterm\n",
    "# Refer this page to setup ollama -- https://medium.com/@abonia/running-ollama-in-google-colab-free-tier-545609258453\n",
    "# curl https://ollama.ai/install.sh | sh\n",
    "# ollama serve &\n",
    "# ollama pull mistral:instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef052d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ollama list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f83c878",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U langchain-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e312407",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72f6c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test connection to Ollama\n",
    "response = ollama.generate(model='mistral:instruct',\n",
    "                          prompt='Give me a brief overview of what Ollama is.',\n",
    "                          options={'temperature': 0.1})\n",
    "print(response['response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c784be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install data processing tools\n",
    "!pip install --quiet kaggle openpyxl stix2 taxii2-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a797bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required modules\n",
    "import networkx as nx\n",
    "import nx_arangodb as nxadb\n",
    "from arango import ArangoClient\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from random import randint\n",
    "import re\n",
    "from langchain_community.graphs import ArangoGraph\n",
    "from langchain_community.chains.graph_qa.arangodb import ArangoGraphQAChain\n",
    "from langchain_core.tools import tool\n",
    "from langchain.agents import initialize_agent, AgentType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc1de78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain.agents import AgentExecutor, Tool\n",
    "from langchain.agents import create_react_agent\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "from langchain.tools import tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c1c77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional imports for functionality\n",
    "import json\n",
    "from datetime import datetime\n",
    "from IPython.display import display, HTML\n",
    "import warnings\n",
    "import os\n",
    "import time\n",
    "import requests\n",
    "import io\n",
    "import zipfile\n",
    "from langchain_community.llms import LlamaCpp\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfc7319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import cuGraph - new imports for GPU acceleration\n",
    "import cugraph\n",
    "import cudf\n",
    "from cugraph.structure.graph_implementation.simpleGraph import simpleGraphImpl\n",
    "import cupy as cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd39cf4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silence warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39e67d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# Initialize Local LLM\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3073134d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the LLM to use Ollama\n",
    "llm = Ollama(\n",
    "    model=\"mistral:instruct\",\n",
    "    temperature=0.1,\n",
    "    base_url=\"http://localhost:11434\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef83b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the LLM\n",
    "test_response = llm.invoke(\"What is a cybersecurity threat graph?\")\n",
    "print(\"\\nLLM Test Response:\")\n",
    "print(test_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab8698b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# Connect to ArangoDB\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64491605",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import userdata\n",
    "ARANGO_URL = userdata.get('ARANGO_URL')\n",
    "ARANGO_USERNAME = userdata.get('ARANGO_USERNAME')\n",
    "ARANGO_PASSWORD = userdata.get('ARANGO_PASSWORD')\n",
    "ARANGO_DB = userdata.get('ARANGO_DB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4167734f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "ARANGO_DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1637eee",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Connect to ArangoDB\n",
    "def connect_to_arangodb():\n",
    "    try:\n",
    "        # Connect to ArangoDB\n",
    "        client = ArangoClient(hosts=ARANGO_URL)\n",
    "\n",
    "        # Connect to the system database to create cyberthreat_db if it doesn't exist\n",
    "        sys_db = client.db(\"_system\", username=ARANGO_USERNAME, password=ARANGO_PASSWORD, verify=True)\n",
    "\n",
    "        # Create a new database if it doesn't exist\n",
    "        if not sys_db.has_database(ARANGO_DB):\n",
    "            sys_db.create_database(ARANGO_DB)\n",
    "            print(f\"Created database: {ARANGO_DB}\")\n",
    "\n",
    "        # Connect to the cyberthreat database\n",
    "        db = client.db(ARANGO_DB, username=ARANGO_USERNAME, password=ARANGO_PASSWORD, verify=True)\n",
    "\n",
    "        print(f\"Successfully connected to ArangoDB at {ARANGO_URL}\")\n",
    "        return db, client\n",
    "    except Exception as e:\n",
    "        print(f\"Error connecting to ArangoDB: {e}\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd814d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "db, client = connect_to_arangodb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e5725e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "\"\"\"# Download and Process CVE and MITRE ATT&CK Data\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2802cf73",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Download CVE data from Kaggle\n",
    "def download_cve_data():\n",
    "    # First, set up Kaggle API credentials if needed\n",
    "    # You may need to upload your kaggle.json file to Colab\n",
    "    if not os.path.exists('/root/.kaggle'):\n",
    "        os.makedirs('/root/.kaggle')\n",
    "\n",
    "    # Check if we already have the data\n",
    "    if not os.path.exists('cve_data'):\n",
    "        os.makedirs('cve_data')\n",
    "\n",
    "    if not os.path.exists('cve_data/cve.csv'):\n",
    "        print(\"Downloading CVE dataset from Kaggle...\")\n",
    "        try:\n",
    "            # Try using Kaggle API if credentials are available\n",
    "            import kaggle\n",
    "            kaggle.api.authenticate()\n",
    "            kaggle.api.dataset_download_files('andrewkronser/cve-common-vulnerabilities-and-exposures', path='cve_data', unzip=True)\n",
    "        except:\n",
    "            # If Kaggle API doesn't work, use direct download\n",
    "            print(\"Kaggle API not available, downloading from alternative source...\")\n",
    "            cve_url = \"https://www.cisa.gov/sites/default/files/csv/known_exploited_vulnerabilities.csv\"\n",
    "            response = requests.get(cve_url)\n",
    "            with open('cve_data/cve.csv', 'wb') as f:\n",
    "                f.write(response.content)\n",
    "\n",
    "    print(\"Loading CVE data...\")\n",
    "    # Load the main CVE data\n",
    "    try:\n",
    "        cve_df = pd.read_csv('cve_data/cve.csv')\n",
    "        print(f\"Successfully loaded CVE data with {len(cve_df)} entries\")\n",
    "        return cve_df\n",
    "    except:\n",
    "        # If main dataset fails, try CISA KEV as fallback\n",
    "        try:\n",
    "            cve_df = pd.read_csv('cve_data/known_exploited_vulnerabilities.csv')\n",
    "            print(f\"Successfully loaded CISA KEV data with {len(cve_df)} entries\")\n",
    "            return cve_df\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading CVE data: {e}\")\n",
    "            # Create a minimal dataset as fallback\n",
    "            return pd.DataFrame({\n",
    "                'cve_id': ['CVE-2023-0001', 'CVE-2023-0002', 'CVE-2023-0003'],\n",
    "                'description': [\n",
    "                    'Remote code execution vulnerability in web server',\n",
    "                    'SQL injection vulnerability in database application',\n",
    "                    'Cross-site scripting vulnerability in web application'\n",
    "                ],\n",
    "                'published_date': ['2023-01-15', '2023-02-20', '2023-03-10'],\n",
    "                'cvss_score': [9.8, 8.5, 7.2]\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d07c35",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def download_mitre_attack_data():\n",
    "    # Check if we already have the data\n",
    "    if not os.path.exists('mitre_data'):\n",
    "        os.makedirs('mitre_data')\n",
    "\n",
    "    if not os.path.exists('mitre_data/enterprise-attack.json'):\n",
    "        print(\"Downloading MITRE ATT&CK Enterprise framework...\")\n",
    "        url = \"https://raw.githubusercontent.com/mitre/cti/master/enterprise-attack/enterprise-attack.json\"\n",
    "        response = requests.get(url)\n",
    "        with open('mitre_data/enterprise-attack.json', 'wb') as f:\n",
    "            f.write(response.content)\n",
    "\n",
    "    print(\"Loading MITRE ATT&CK data...\")\n",
    "    # Load the MITRE ATT&CK STIX data\n",
    "    try:\n",
    "        # Explicitly use UTF-8 encoding when reading the file\n",
    "        with open('mitre_data/enterprise-attack.json', 'r', encoding='utf-8') as f:\n",
    "            attack_data = json.load(f)\n",
    "        print(f\"Successfully loaded MITRE ATT&CK data with {len(attack_data['objects'])} objects\")\n",
    "        return attack_data\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading MITRE ATT&CK data: {str(e)}\")\n",
    "\n",
    "        # Try a more robust approach with error handling\n",
    "        try:\n",
    "            print(\"Trying alternative loading approach...\")\n",
    "            import codecs\n",
    "            with codecs.open('mitre_data/enterprise-attack.json', 'r', encoding='utf-8', errors='replace') as f:\n",
    "                content = f.read()\n",
    "                attack_data = json.loads(content)\n",
    "            print(f\"Successfully loaded MITRE ATT&CK data with alternative method\")\n",
    "            return attack_data\n",
    "        except Exception as e2:\n",
    "            print(f\"Alternative loading also failed: {str(e2)}\")\n",
    "\n",
    "            # Create minimal dataset as fallback\n",
    "            print(\"Using fallback MITRE data\")\n",
    "            return {\n",
    "                'objects': [\n",
    "                    {\n",
    "                        'type': 'attack-pattern',\n",
    "                        'id': 'attack-pattern--t1059',\n",
    "                        'name': 'Command and Scripting Interpreter',\n",
    "                        'description': 'Adversaries may abuse command and script interpreters to execute commands',\n",
    "                        'kill_chain_phases': [{'kill_chain_name': 'mitre-attack', 'phase_name': 'execution'}]\n",
    "                    },\n",
    "                    {\n",
    "                        'type': 'attack-pattern',\n",
    "                        'id': 'attack-pattern--t1566',\n",
    "                        'name': 'Phishing',\n",
    "                        'description': 'Adversaries may send phishing messages to gain access to victim systems',\n",
    "                        'kill_chain_phases': [{'kill_chain_name': 'mitre-attack', 'phase_name': 'initial-access'}]\n",
    "                    },\n",
    "                    {\n",
    "                        'type': 'intrusion-set',\n",
    "                        'id': 'intrusion-set--apt28',\n",
    "                        'name': 'APT28',\n",
    "                        'description': 'APT28 is a threat group that has been attributed to Russia',\n",
    "                    }\n",
    "                ]\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e07f16",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Process MITRE ATT&CK data into suitable format for ArangoDB\n",
    "def process_mitre_attack_data(attack_data):\n",
    "    # Lists to store processed data\n",
    "    techniques = []\n",
    "    tactics = []\n",
    "    threat_actors = []\n",
    "    relationships = []\n",
    "\n",
    "    # Process objects from MITRE ATT&CK data\n",
    "    for obj in attack_data['objects']:\n",
    "        if obj['type'] == 'attack-pattern':\n",
    "            # This is a technique\n",
    "            technique_id = obj.get('external_references', [{}])[0].get('external_id', '').upper() if 'external_references' in obj else None\n",
    "            if not technique_id:\n",
    "                continue\n",
    "\n",
    "            tactic = ''\n",
    "            if 'kill_chain_phases' in obj:\n",
    "                for phase in obj['kill_chain_phases']:\n",
    "                    if phase.get('kill_chain_name') == 'mitre-attack':\n",
    "                        tactic = phase.get('phase_name', '').replace('-', ' ').title()\n",
    "                        break\n",
    "\n",
    "            techniques.append({\n",
    "                '_key': technique_id,\n",
    "                'name': obj.get('name', ''),\n",
    "                'tactic': tactic,\n",
    "                'description': obj.get('description', '')\n",
    "            })\n",
    "\n",
    "        elif obj['type'] == 'intrusion-set':\n",
    "            # This is a threat actor/group\n",
    "            actor_id = obj['id'].split('--')[1]\n",
    "            threat_actors.append({\n",
    "                '_key': actor_id,\n",
    "                'name': obj.get('name', ''),\n",
    "                'type': 'Nation State' if 'government' in obj.get('description', '').lower() else 'Criminal',\n",
    "                'description': obj.get('description', '')\n",
    "            })\n",
    "\n",
    "        elif obj['type'] == 'relationship':\n",
    "            # This is a relationship between objects\n",
    "            if obj.get('relationship_type') == 'uses' and obj.get('source_ref', '').startswith('intrusion-set') and obj.get('target_ref', '').startswith('attack-pattern'):\n",
    "                source_id = obj['source_ref'].split('--')[1]\n",
    "                target_id = obj['target_ref'].split('--')[1]\n",
    "\n",
    "                # Get external ID for technique if available\n",
    "                for rel_obj in attack_data['objects']:\n",
    "                    if rel_obj.get('id') == obj['target_ref'] and 'external_references' in rel_obj:\n",
    "                        for ref in rel_obj['external_references']:\n",
    "                            if ref.get('source_name') == 'mitre-attack':\n",
    "                                target_id = ref.get('external_id', '').upper()\n",
    "                                break\n",
    "\n",
    "                relationships.append({\n",
    "                    'source': source_id,\n",
    "                    'target': target_id,\n",
    "                    'type': 'uses'\n",
    "                })\n",
    "\n",
    "    return {\n",
    "        'techniques': techniques,\n",
    "        'threat_actors': threat_actors,\n",
    "        'relationships': relationships\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213a49f6",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Process CVE data\n",
    "def process_cve_data(cve_df, limit=1000):\n",
    "    print(f\"Processing {len(cve_df)} CVE entries...\")\n",
    "\n",
    "    # Map the CISA KEV column names to our expected column names\n",
    "    if 'cveID' in cve_df.columns and 'CVE ID' not in cve_df.columns:\n",
    "        print(\"Detected CISA KEV format, mapping columns...\")\n",
    "        cve_df = cve_df.rename(columns={\n",
    "            'cveID': 'CVE ID',\n",
    "            'shortDescription': 'Description',\n",
    "            'dateAdded': 'Published',\n",
    "            'cwes': 'CWE ID'\n",
    "        })\n",
    "\n",
    "        # Add a default CVSS Score column if missing\n",
    "        if 'CVSS Score' not in cve_df.columns:\n",
    "            print(\"Adding default CVSS scores based on CWE...\")\n",
    "            # Assign a default CVSS based on the shortDescription seriousness\n",
    "            def estimate_cvss(row):\n",
    "                desc = row['Description'].lower() if isinstance(row['Description'], str) else ''\n",
    "                if any(term in desc for term in ['critical', 'remote code execution', 'rce']):\n",
    "                    return 9.0\n",
    "                elif any(term in desc for term in ['high', 'arbitrary code', 'privilege escalation']):\n",
    "                    return 7.5\n",
    "                elif any(term in desc for term in ['medium', 'information disclosure', 'cross-site']):\n",
    "                    return 5.0\n",
    "                else:\n",
    "                    return 4.0\n",
    "\n",
    "            cve_df['CVSS Score'] = cve_df.apply(estimate_cvss, axis=1)\n",
    "\n",
    "    # Process into the format for ArangoDB\n",
    "    vulnerabilities = []\n",
    "\n",
    "    for _, row in cve_df.iterrows():\n",
    "        cve_id = row.get('CVE ID', '')\n",
    "        if not cve_id:\n",
    "            continue\n",
    "\n",
    "        # Clean up the CVE ID to use as a key\n",
    "        cve_key = cve_id.replace('-', '_')\n",
    "\n",
    "        # Determine severity based on CVSS if available\n",
    "        cvss_score = row.get('CVSS Score', 0)\n",
    "        if not isinstance(cvss_score, (int, float)):\n",
    "            try:\n",
    "                cvss_score = float(cvss_score)\n",
    "            except:\n",
    "                cvss_score = 0\n",
    "\n",
    "        severity = 'Unknown'\n",
    "        if cvss_score >= 9.0:\n",
    "            severity = 'Critical'\n",
    "        elif cvss_score >= 7.0:\n",
    "            severity = 'High'\n",
    "        elif cvss_score >= 4.0:\n",
    "            severity = 'Medium'\n",
    "        elif cvss_score > 0:\n",
    "            severity = 'Low'\n",
    "\n",
    "        # Parse the published date\n",
    "        published_date = row.get('Published', '')\n",
    "\n",
    "        vulnerabilities.append({\n",
    "            '_key': cve_key,\n",
    "            'name': cve_id,\n",
    "            'description': row.get('Description', ''),\n",
    "            'cvss_score': cvss_score,\n",
    "            'published_date': published_date,\n",
    "            'severity': severity,\n",
    "            'cwe_id': row.get('CWE ID', '')\n",
    "        })\n",
    "\n",
    "    print(f\"Created {len(vulnerabilities)} vulnerability objects\")\n",
    "    return vulnerabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6584817a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Generate relationships between entities with improved connectivity\n",
    "def generate_relationships(vulnerabilities, threat_actors, techniques):\n",
    "    \"\"\"Generate realistic relationships between CVEs, assets, and threat actors with improved connectivity\"\"\"\n",
    "    # Define common asset types in organizations\n",
    "    assets = [\n",
    "        {\"_key\": \"server001\", \"name\": \"Production Web Server\", \"type\": \"Server\", \"criticality\": \"High\", \"operating_system\": \"Ubuntu 20.04\", \"ip_address\": \"10.0.0.1\"},\n",
    "        {\"_key\": \"server002\", \"name\": \"Database Server\", \"type\": \"Server\", \"criticality\": \"Critical\", \"operating_system\": \"CentOS 8\", \"ip_address\": \"10.0.0.2\"},\n",
    "        {\"_key\": \"server003\", \"name\": \"Test Web Server\", \"type\": \"Server\", \"criticality\": \"Low\", \"operating_system\": \"Ubuntu 20.04\", \"ip_address\": \"10.0.0.3\"},\n",
    "        {\"_key\": \"server004\", \"name\": \"Email Server\", \"type\": \"Server\", \"criticality\": \"High\", \"operating_system\": \"Windows Server 2019\", \"ip_address\": \"10.0.0.4\"},\n",
    "        {\"_key\": \"server005\", \"name\": \"Domain Controller\", \"type\": \"Server\", \"criticality\": \"Critical\", \"operating_system\": \"Windows Server 2019\", \"ip_address\": \"10.0.0.5\"},\n",
    "        {\"_key\": \"workstation001\", \"name\": \"CEO Laptop\", \"type\": \"Endpoint\", \"criticality\": \"Medium\", \"operating_system\": \"Windows 11\", \"ip_address\": \"10.0.1.1\"},\n",
    "        {\"_key\": \"workstation002\", \"name\": \"CFO Laptop\", \"type\": \"Endpoint\", \"criticality\": \"Medium\", \"operating_system\": \"MacOS\", \"ip_address\": \"10.0.1.2\"},\n",
    "        {\"_key\": \"workstation003\", \"name\": \"Developer Workstation\", \"type\": \"Endpoint\", \"criticality\": \"Low\", \"operating_system\": \"Ubuntu 22.04\", \"ip_address\": \"10.0.1.3\"},\n",
    "        {\"_key\": \"router001\", \"name\": \"Main Router\", \"type\": \"Network\", \"criticality\": \"High\", \"operating_system\": \"Cisco IOS\", \"ip_address\": \"10.0.0.254\"},\n",
    "        {\"_key\": \"firewall001\", \"name\": \"Perimeter Firewall\", \"type\": \"Network\", \"criticality\": \"Critical\", \"operating_system\": \"Palo Alto PAN-OS\", \"ip_address\": \"10.0.0.253\"}\n",
    "    ]\n",
    "\n",
    "    # Create relationships\n",
    "    exploits = []  # ThreatActors -> Vulnerabilities\n",
    "    targets = []   # Vulnerabilities -> Assets\n",
    "    uses = []      # ThreatActors -> Techniques\n",
    "\n",
    "    import random\n",
    "    from datetime import datetime, timedelta\n",
    "\n",
    "    # Get lists of keys\n",
    "    vuln_keys = [v['_key'] for v in vulnerabilities]\n",
    "    actor_keys = [a['_key'] for a in threat_actors]\n",
    "    technique_keys = [t['_key'] for t in techniques]\n",
    "    asset_keys = [a['_key'] for a in assets]\n",
    "\n",
    "    # Filter vulnerabilities by severity - actors prefer high impact vulns\n",
    "    critical_vulns = [v['_key'] for v in vulnerabilities if v.get('severity', '') in ['Critical', 'High']]\n",
    "    if not critical_vulns:\n",
    "        critical_vulns = vuln_keys\n",
    "\n",
    "    # Create actor -> vulnerability (exploits) edges\n",
    "    # Enhanced: Each actor exploits more vulnerabilities for better connectivity\n",
    "    for actor in threat_actors:\n",
    "        # Nation state actors exploit more vulnerabilities than criminal groups\n",
    "        # Increased numbers for better connectivity\n",
    "        num_exploits = random.randint(5, 12) if actor.get('type', '') == 'Nation State' else random.randint(3, 8)\n",
    "\n",
    "        # Select vulnerabilities to exploit - prefer critical/high but also some random ones\n",
    "        exploit_vulns = random.sample(critical_vulns, min(num_exploits // 2 + 1, len(critical_vulns)))\n",
    "        exploit_vulns += random.sample(vuln_keys, min(num_exploits - len(exploit_vulns), len(vuln_keys)))\n",
    "\n",
    "        for vuln_key in exploit_vulns:\n",
    "            # Generate a random date in the last year\n",
    "            days_ago = random.randint(30, 365)\n",
    "            date_observed = (datetime.now() - timedelta(days=days_ago)).strftime('%Y-%m-%d')\n",
    "\n",
    "            confidence = random.choice(['High', 'Medium', 'Low'])\n",
    "\n",
    "            exploits.append({\n",
    "                \"_from\": f\"ThreatActors/{actor['_key']}\",\n",
    "                \"_to\": f\"Vulnerabilities/{vuln_key}\",\n",
    "                \"confidence\": confidence,\n",
    "                \"date_observed\": date_observed\n",
    "            })\n",
    "\n",
    "    # Create vulnerability -> asset (targets) edges\n",
    "    # Enhanced: Each vulnerability affects more assets\n",
    "    for vuln in vulnerabilities:\n",
    "        # Determine how many assets this vulnerability affects\n",
    "        # More severe vulnerabilities tend to affect more assets\n",
    "        # Increased numbers for better connectivity\n",
    "        if vuln.get('severity', '') == 'Critical':\n",
    "            num_affected = random.randint(3, 7)\n",
    "        elif vuln.get('severity', '') == 'High':\n",
    "            num_affected = random.randint(2, 5)\n",
    "        else:\n",
    "            num_affected = random.randint(1, 3)\n",
    "\n",
    "        # Select assets to affect\n",
    "        affected_assets = random.sample(asset_keys, min(num_affected, len(asset_keys)))\n",
    "\n",
    "        for asset_key in affected_assets:\n",
    "            # Determine impact based on vulnerability severity and asset criticality\n",
    "            asset = next((a for a in assets if a['_key'] == asset_key), None)\n",
    "            if not asset:\n",
    "                continue\n",
    "\n",
    "            if vuln.get('severity', '') == 'Critical' and asset['criticality'] == 'Critical':\n",
    "                impact = 'Critical'\n",
    "            elif vuln.get('severity', '') == 'Critical' or asset['criticality'] == 'Critical':\n",
    "                impact = 'High'\n",
    "            elif vuln.get('severity', '') == 'High' and asset['criticality'] == 'High':\n",
    "                impact = 'High'\n",
    "            elif vuln.get('severity', '') == 'High' or asset['criticality'] == 'High':\n",
    "                impact = 'Medium'\n",
    "            else:\n",
    "                impact = 'Low'\n",
    "\n",
    "            # Determine remediation status\n",
    "            status_weights = {\n",
    "                'Patched': 30,\n",
    "                'In Progress': 30,\n",
    "                'Unpatched': 40\n",
    "            }\n",
    "            statuses = list(status_weights.keys())\n",
    "            weights = list(status_weights.values())\n",
    "\n",
    "            remediation_status = random.choices(statuses, weights=weights, k=1)[0]\n",
    "\n",
    "            targets.append({\n",
    "                \"_from\": f\"Vulnerabilities/{vuln['_key']}\",\n",
    "                \"_to\": f\"Assets/{asset_key}\",\n",
    "                \"impact\": impact,\n",
    "                \"remediation_status\": remediation_status\n",
    "            })\n",
    "\n",
    "    # Create actor -> technique (uses) edges\n",
    "    # Enhanced: Each actor uses more techniques\n",
    "    for actor in threat_actors:\n",
    "        # Determine how many techniques this actor uses\n",
    "        # Increased for better connectivity\n",
    "        num_techniques = random.randint(4, 10)\n",
    "\n",
    "        # Select techniques\n",
    "        used_techniques = random.sample(technique_keys, min(num_techniques, len(technique_keys)))\n",
    "\n",
    "        for tech_key in used_techniques:\n",
    "            # Generate a random date in the last year\n",
    "            days_ago = random.randint(30, 365)\n",
    "            last_observed = (datetime.now() - timedelta(days=days_ago)).strftime('%Y-%m-%d')\n",
    "\n",
    "            frequency = random.choice(['High', 'Medium', 'Low'])\n",
    "\n",
    "            uses.append({\n",
    "                \"_from\": f\"ThreatActors/{actor['_key']}\",\n",
    "                \"_to\": f\"Techniques/{tech_key}\",\n",
    "                \"frequency\": frequency,\n",
    "                \"last_observed\": last_observed\n",
    "            })\n",
    "\n",
    "    # Ensure all vulnerability to asset paths connect to at least one threat actor\n",
    "    # This ensures the completeness of attack paths\n",
    "    return {\n",
    "        \"assets\": assets,\n",
    "        \"exploits\": exploits,\n",
    "        \"targets\": targets,\n",
    "        \"uses\": uses\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7050ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and process the datasets\n",
    "print(\"Downloading and processing CVE data...\")\n",
    "cve_df = download_cve_data()\n",
    "cve_vulnerabilities = process_cve_data(cve_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12288053",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Downloading and processing MITRE ATT&CK data...\")\n",
    "attack_data = download_mitre_attack_data()\n",
    "processed_attack = process_mitre_attack_data(attack_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e87408f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate relationships between the entities\n",
    "relationships = generate_relationships(\n",
    "    cve_vulnerabilities,\n",
    "    processed_attack['threat_actors'],\n",
    "    processed_attack['techniques']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7accf33",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Processed data summary:\")\n",
    "print(f\"- {len(cve_vulnerabilities)} CVE vulnerabilities\")\n",
    "print(f\"- {len(processed_attack['techniques'])} MITRE techniques\")\n",
    "print(f\"- {len(processed_attack['threat_actors'])} threat actors\")\n",
    "print(f\"- {len(relationships['assets'])} assets\")\n",
    "print(f\"- {len(relationships['exploits'])} exploit relationships\")\n",
    "print(f\"- {len(relationships['targets'])} target relationships\")\n",
    "print(f\"- {len(relationships['uses'])} technique usage relationships\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6b5b99",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "\"\"\"# Create Cybersecurity Graph Schema\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b0f9c1",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Set up collections for the threat graph\n",
    "def setup_threat_graph_collections(db):\n",
    "    # Define collections needed for cybersecurity graph\n",
    "    collections = [\n",
    "        {\"name\": \"Vulnerabilities\", \"edge\": False},\n",
    "        {\"name\": \"Assets\", \"edge\": False},\n",
    "        {\"name\": \"ThreatActors\", \"edge\": False},\n",
    "        {\"name\": \"Techniques\", \"edge\": False},\n",
    "        {\"name\": \"Exploits\", \"edge\": True},\n",
    "        {\"name\": \"Targets\", \"edge\": True},\n",
    "        {\"name\": \"Uses\", \"edge\": True}\n",
    "    ]\n",
    "\n",
    "    # Create collections if they don't exist\n",
    "    for col in collections:\n",
    "        if not db.has_collection(col[\"name\"]):\n",
    "            db.create_collection(col[\"name\"], edge=col[\"edge\"])\n",
    "            print(f\"Created {col['name']} {'edge ' if col['edge'] else ''}collection\")\n",
    "\n",
    "    # Create a named graph if it doesn't exist\n",
    "    if not db.has_graph(\"ThreatGraph\"):\n",
    "        graph = db.create_graph(\"ThreatGraph\")\n",
    "\n",
    "        # Define edge definitions\n",
    "        graph.create_edge_definition(\n",
    "            edge_collection=\"Exploits\",\n",
    "            from_vertex_collections=[\"ThreatActors\"],\n",
    "            to_vertex_collections=[\"Vulnerabilities\"]\n",
    "        )\n",
    "\n",
    "        graph.create_edge_definition(\n",
    "            edge_collection=\"Targets\",\n",
    "            from_vertex_collections=[\"Vulnerabilities\"],\n",
    "            to_vertex_collections=[\"Assets\"]\n",
    "        )\n",
    "\n",
    "        graph.create_edge_definition(\n",
    "            edge_collection=\"Uses\",\n",
    "            from_vertex_collections=[\"ThreatActors\"],\n",
    "            to_vertex_collections=[\"Techniques\"]\n",
    "        )\n",
    "\n",
    "        print(\"Created ThreatGraph with edge definitions\")\n",
    "    else:\n",
    "        graph = db.graph(\"ThreatGraph\")\n",
    "        print(\"Using existing ThreatGraph\")\n",
    "\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b0a874",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Load data into ArangoDB\n",
    "def load_threat_data_to_arangodb(db, vulnerabilities, techniques, threat_actors, assets, exploits, targets, uses):\n",
    "    # Load vertex collections\n",
    "    collections_data = {\n",
    "        \"Vulnerabilities\": vulnerabilities,\n",
    "        \"Assets\": assets,\n",
    "        \"ThreatActors\": threat_actors,\n",
    "        \"Techniques\": techniques\n",
    "    }\n",
    "\n",
    "    for collection_name, data in collections_data.items():\n",
    "        if db.collection(collection_name).count() == 0:\n",
    "            print(f\"Loading {len(data)} documents into {collection_name}...\")\n",
    "            # Load in batches to avoid memory issues\n",
    "            batch_size = 1000\n",
    "            for i in range(0, len(data), batch_size):\n",
    "                batch = data[i:i+batch_size]\n",
    "                db.collection(collection_name).import_bulk(batch)\n",
    "            print(f\"Loaded data into {collection_name}\")\n",
    "        else:\n",
    "            print(f\"Collection {collection_name} already has data, skipping import\")\n",
    "\n",
    "    # Load edge collections\n",
    "    edge_collections_data = {\n",
    "        \"Exploits\": exploits,\n",
    "        \"Targets\": targets,\n",
    "        \"Uses\": uses\n",
    "    }\n",
    "\n",
    "    for collection_name, data in edge_collections_data.items():\n",
    "        if db.collection(collection_name).count() == 0:\n",
    "            print(f\"Loading {len(data)} edges into {collection_name}...\")\n",
    "            # Load in batches to avoid memory issues\n",
    "            batch_size = 1000\n",
    "            for i in range(0, len(data), batch_size):\n",
    "                batch = data[i:i+batch_size]\n",
    "                db.collection(collection_name).import_bulk(batch)\n",
    "            print(f\"Loaded data into {collection_name}\")\n",
    "        else:\n",
    "            print(f\"Collection {collection_name} already has data, skipping import\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a089d887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the graph and load the data\n",
    "graph = setup_threat_graph_collections(db)\n",
    "load_threat_data_to_arangodb(\n",
    "    db,\n",
    "    cve_vulnerabilities,\n",
    "    processed_attack['techniques'],\n",
    "    processed_attack['threat_actors'],\n",
    "    relationships['assets'],\n",
    "    relationships['exploits'],\n",
    "    relationships['targets'],\n",
    "    relationships['uses']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281cbf43",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "\"\"\"# Create NetworkX and cuGraph from ArangoDB\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b34b447",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Create NetworkX graph from ArangoDB\n",
    "def create_threat_graph_nx():\n",
    "    try:\n",
    "        # Create a nx-arangodb graph directly connected to the ArangoDB graph\n",
    "        G_threat = nxadb.Graph(\n",
    "            name=\"ThreatGraph\",\n",
    "            db=db,\n",
    "            create=False  # Don't create a new graph, use existing one\n",
    "        )\n",
    "\n",
    "        print(f\"Successfully connected to ThreatGraph in ArangoDB\")\n",
    "        print(f\"Graph has {G_threat.number_of_nodes()} nodes and {G_threat.number_of_edges()} edges\")\n",
    "\n",
    "        return G_threat\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating ThreatGraph from ArangoDB: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba72d0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New function to convert NetworkX graph to cuGraph\n",
    "def convert_to_cugraph(G_nx):\n",
    "    \"\"\"\n",
    "    Convert a NetworkX graph to a cuGraph graph for GPU-accelerated analytics\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"Converting NetworkX graph to cuGraph...\")\n",
    "        \n",
    "        # Create edge list dataframe for cuGraph\n",
    "        sources = []\n",
    "        destinations = []\n",
    "        weights = []\n",
    "        edge_attrs = []\n",
    "        \n",
    "        # Create node mappings (to handle string node IDs)\n",
    "        node_map = {node: i for i, node in enumerate(G_nx.nodes())}\n",
    "        reverse_node_map = {i: node for node, i in node_map.items()}\n",
    "        \n",
    "        # Store node attributes \n",
    "        node_attrs = {node_map[node]: attrs for node, attrs in G_nx.nodes(data=True)}\n",
    "        \n",
    "        # Extract edges and their attributes\n",
    "        for source, target, data in G_nx.edges(data=True):\n",
    "            sources.append(node_map[source])\n",
    "            destinations.append(node_map[target])\n",
    "            \n",
    "            # For simplicity, use weight=1.0 for all edges\n",
    "            weights.append(1.0)\n",
    "            \n",
    "            # Store edge attributes for later reference\n",
    "            edge_attrs.append(data)\n",
    "            \n",
    "        # Create cuDF DataFrame for edges\n",
    "        df = cudf.DataFrame()\n",
    "        df['src'] = sources\n",
    "        df['dst'] = destinations\n",
    "        df['weight'] = weights\n",
    "        \n",
    "        # Create cuGraph from DataFrame\n",
    "        G_cu = cugraph.Graph()\n",
    "        G_cu.from_cudf_edgelist(df, source='src', destination='dst', edge_attr='weight', renumber=False)\n",
    "        \n",
    "        # Store metadata for mapping back to original graph\n",
    "        G_cu.node_map = node_map\n",
    "        G_cu.reverse_node_map = reverse_node_map\n",
    "        G_cu.node_attrs = node_attrs\n",
    "        G_cu.edge_attrs = edge_attrs\n",
    "        \n",
    "        print(f\"Successfully converted to cuGraph with {G_cu.number_of_vertices()} vertices and {G_cu.number_of_edges()} edges\")\n",
    "        return G_cu\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting to cuGraph: {e}\")\n",
    "        print(\"Falling back to NetworkX graph\")\n",
    "        return None"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# -*- coding: utf-8 -*-",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
